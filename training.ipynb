{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28db568",
   "metadata": {},
   "source": [
    "## Training/generating and other long running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b388198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import settings.config as cfg\n",
    "\n",
    "preprocessed_dataset_folder = cfg.preprocessed_dataset_folder\n",
    "group_sizes_to_create = cfg.group_sizes_to_create\n",
    "group_similarity_to_create = cfg.group_similarity_to_create\n",
    "group_number = cfg.group_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982ebf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942220</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942221</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942222</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942223</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942224</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>942225 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  rating\n",
       "0          1  1193       5\n",
       "1          1   661       3\n",
       "2          1   914       3\n",
       "3          1  3408       4\n",
       "4          1  2355       5\n",
       "...      ...   ...     ...\n",
       "942220  6040  1091       1\n",
       "942221  6040  1094       5\n",
       "942222  6040   562       5\n",
       "942223  6040  1096       4\n",
       "942224  6040  1097       4\n",
       "\n",
       "[942225 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ratings_df = pd.read_csv(preprocessed_dataset_folder+\"/ratings.csv\")\n",
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289955e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the split immediately so that we can generate groups based on similarity on training data only, vs on full data\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42, shuffle=True, stratify=ratings_df[\"user\"])\n",
    "pickle.dump(train_df, open(preprocessed_dataset_folder+\"/train_df.pkl\", \"wb\"))\n",
    "pickle.dump(test_df, open(preprocessed_dataset_folder+\"/test_df.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7241ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit.algorithms.als import BiasedMF, ImplicitMF\n",
    "from lenskit.algorithms import Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83811f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_mf = BiasedMF(features=30, iterations=20, reg=0.1, rng_spec=42)                \n",
    "recsys_biased_mf = Recommender.adapt(biased_mf)\n",
    "recsys_biased_mf = recsys_biased_mf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_mf = ImplicitMF(features=30, iterations=20, reg=0.1, rng_spec=42)               \n",
    "recsys_implicit_mf = Recommender.adapt(implicit_mf)\n",
    "recsys_implicit_mf = recsys_implicit_mf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(recsys_biased_mf, open(preprocessed_dataset_folder+\"/biasedMf.pkl\", \"wb\"))\n",
    "pickle.dump(recsys_implicit_mf, open(preprocessed_dataset_folder+\"/implicitMf.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c249c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bebb5336",
   "metadata": {},
   "source": [
    "## Groups generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb80e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline_groups(sm, sm_text, sim_threshold_override):\n",
    "    \n",
    "    old_sim_threshold = cfg.similar_threshold\n",
    "    cfg.similar_threshold = sim_threshold_override\n",
    "        \n",
    "    print(f\"####################### Using similarity matrix = {sm_text}\")\n",
    "    from synthetic_groups_generation.groups_generators import GroupsGenerator\n",
    "\n",
    "    group_list = list()\n",
    "    for group_type in group_similarity_to_create:\n",
    "        print(\"###########################################################################\")\n",
    "        print(f\"#################### group_type={group_type} #############################\")\n",
    "        print(\"###########################################################################\")\n",
    "        grpGenerator = GroupsGenerator.getGroupsGenerator(group_type)\n",
    "        current_list = grpGenerator.generateGroups(user_id_indexes, user_id_set, sm.copy(), group_sizes_to_create, group_number)\n",
    "\n",
    "        display(pd.DataFrame.from_records(current_list))\n",
    "\n",
    "        group_list = group_list + current_list\n",
    "        \n",
    "\n",
    "    cfg.similar_threshold = old_sim_threshold\n",
    "    \n",
    "    return group_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4bf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dict = {\n",
    "    \"sim_full\": sim_matrix_full,\n",
    "    \"sim_train\": sim_matrix_train,\n",
    "    \"sim_biased_mf_cos\": sim_matrix_biased_mf_cos,\n",
    "    \"sim_biased_mf_l2\": sim_matrix_biased_mf_l2,\n",
    "    \"sim_implicit_mf_cos\": sim_matrix_implicit_mf_cos,\n",
    "    \"sim_implicit_mf_l2\": sim_matrix_implicit_mf_l2\n",
    "}\n",
    "\n",
    "similarity_raw_to_name = {\n",
    "    \"Pearson's Correlation Coeficient FULL\": \"sim_full\",\n",
    "    \"Pearson's Correlation Coeficient TRAIN\": \"sim_train\",\n",
    "    \"Biased MF Cosine\": \"sim_biased_mf_cos\",\n",
    "    \"Biased MF L2\": \"sim_biased_mf_l2\",\n",
    "    \"Implicit MF Cosine\": \"sim_implicit_mf_cos\",\n",
    "    \"Implicit MF L2\": \"sim_implicit_mf_l2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "similarity_threshold_dict = {}\n",
    "\n",
    "\n",
    "for sim_name, sim_raw_name in similarity_raw_to_name.items():\n",
    "    sim_matrix = similarity_dict[sim_raw_name]\n",
    "    data = matrix.flatten()\n",
    "    perc = np.percentile(data, [50, 75, 80, 90, 95, 99])\n",
    "    print(f\"Percentiles for {sim_name} = {perc}\")\n",
    "    similarity_threshold_dict[similarity_raw_to_name[name]] = perc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_lists = {}\n",
    "for sim_name, sim_matrix in similarity_dict.items():\n",
    "    group_lists[sim_name] = gen_baseline_groups(sim_matrix, sim_name, sim_threshold_override = similarity_threshold_dict[sim_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b993da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_outlier_groups(group_list, sm, sm_text):\n",
    "    \n",
    "    print(f\"####################### Using similarity matrix = {sm_text}\")\n",
    "    \n",
    "    from synthetic_groups_generation.groups_generators import Outliers\n",
    "    from synthetic_groups_generation.groups_generators import GroupsGenerator\n",
    "    # Add one more group type that needs special handling\n",
    "    # Take similar groups and replace some of its members by outliers (so there is correspondence between the original similarity group and new outlier group)\n",
    "    new_centered_outlier_groups = []\n",
    "    new_diverged_outlier_groups = []\n",
    "\n",
    "    for group in group_list:\n",
    "        if group['group_similarity'] == 'similar':\n",
    "\n",
    "            n_outliers = Outliers.get_num_outliers(group['group_size'])\n",
    "\n",
    "\n",
    "            group_clone = deepcopy(group)\n",
    "            group_clone['group_similarity'] = 'similar_with_centered_outliers'\n",
    "            group_clone['group_members'][:n_outliers] = Outliers.find_outlier_centered_group(group['group_members'][n_outliers:], n_outliers, sm.copy(), user_id_indexes)\n",
    "            group_clone['avg_similarity'] = GroupsGenerator.compute_average_similarity(group_clone['group_members'], user_id_indexes, sm.copy())\n",
    "            new_centered_outlier_groups.append(group_clone)\n",
    "            assert group_clone[\"group_members\"] != group[\"group_members\"], f\"{group_clone['group_members']}, {group['group_members']}\"\n",
    "\n",
    "            group_clone = deepcopy(group)\n",
    "            group_clone['group_similarity'] = 'similar_with_diverged_outliers'\n",
    "            group_clone['group_members'][:n_outliers] = Outliers.find_outlier_diverged_group(group['group_members'][n_outliers:], n_outliers, sm.copy(), user_id_indexes)\n",
    "            group_clone['avg_similarity'] = GroupsGenerator.compute_average_similarity(group_clone['group_members'], user_id_indexes, sm.copy())\n",
    "            new_diverged_outlier_groups.append(group_clone)\n",
    "            assert group_clone[\"group_members\"] != group[\"group_members\"], f\"{group_clone['group_members']}, {group['group_members']}\"\n",
    "\n",
    "            print(f\"lens: {len(new_centered_outlier_groups)}, {len(new_diverged_outlier_groups)}\")\n",
    "        \n",
    "    return {\n",
    "        \"centered_outlier_groups\": new_centered_outlier_groups,\n",
    "        \"diverged_outlier_groups\": new_diverged_outlier_groups\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groups = {}\n",
    "for sim_name, sim_matrix in similarity_dict.items():\n",
    "    new_groups[sim_name] = gen_outlier_groups(group_lists[sim_name], sim_matrix, sim_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centered_outlier_groups, new_diverged_outlier_groups = gen_outlier_groups(sim_matrix_mf, \"MF Sim matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91483e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list.extend(new_centered_outlier_groups)\n",
    "group_list.extend(new_diverged_outlier_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dict = dict()\n",
    "for group_id, group in zip(range(len(group_list)), group_list):\n",
    "    group_dict[group_id] = group\n",
    "display(group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd273c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Dump standard similarity version\n",
    "#pickle.dump(group_dict, open(preprocessed_dataset_folder+\"/group_composition.pkl\", \"wb\"))\n",
    "\n",
    "# Dump MF similarity version\n",
    "#pickle.dump(group_dict, open(preprocessed_dataset_folder+\"/group_composition_mf.pkl\", \"wb\"))\n",
    "\n",
    "for sim_name in similarity_dict.keys():\n",
    "    g = group_lists[sim_name] + new_groups[sim_name][\"centered_outlier_groups\"] + new_groups[sim_name][\"diverged_outlier_groups\"]\n",
    "    \n",
    "    group_dict = dict()\n",
    "    for group_id, group in zip(range(len(g)), g):\n",
    "        group_dict[group_id] = group\n",
    "        \n",
    "    pickle.dump(group_dict, open(preprocessed_dataset_folder+f\"/group_composition_{sim_name}.pkl\", \"wb\"))\n",
    "    \n",
    "    \n",
    "    groups_list = list()\n",
    "    for group in group_dict:\n",
    "        groups_list.append(\n",
    "            {\n",
    "                'group_id': group,\n",
    "                'group_size': group_dict[group]['group_size'],\n",
    "                'group_similarity': group_dict[group]['group_similarity'],\n",
    "                'group_members': group_dict[group]['group_members'],\n",
    "                'avg_similarity': group_dict[group]['avg_similarity']\n",
    "            }\n",
    "        )\n",
    "\n",
    "    groups_df = pd.DataFrame.from_records(groups_list)\n",
    "    groups_df.to_csv(preprocessed_dataset_folder+f\"/group_composition_{sim_name}.csv\")\n",
    "    \n",
    "\n",
    "    print(\"Done with:\", sim_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_list = list()\n",
    "for group in group_dict:\n",
    "    groups_list.append(\n",
    "        {\n",
    "            'group_id': group,\n",
    "            'group_size': group_dict[group]['group_size'],\n",
    "            'group_similarity': group_dict[group]['group_similarity'],\n",
    "            'group_members': group_dict[group]['group_members'],\n",
    "            'avg_similarity': group_dict[group]['avg_similarity']\n",
    "        }\n",
    "    )\n",
    "\n",
    "groups_df = pd.DataFrame.from_records(groups_list)\n",
    "display(groups_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups_df.to_csv(preprocessed_dataset_folder+\"/group_composition.csv\")\n",
    "groups_df.to_csv(preprocessed_dataset_folder+\"/group_composition_mf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6ef66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61c801dd",
   "metadata": {},
   "source": [
    "# GRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import settings.config_movie_lens as cfg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_rows = 500 #Changes the number of rows diplayed (default is 60)\n",
    "\n",
    "preprocessed_dataset_folder = cfg.preprocessed_dataset_folder\n",
    "recommendations_number = cfg.recommendations_number\n",
    "group_types = cfg.group_types\n",
    "\n",
    "cfg.top_k = None # Predict for everything, do not restrict anything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f3432",
   "metadata": {},
   "source": [
    "## Train individual RS / Prepare groundtruth\n",
    "Takes long time, only needed if you don't have test_pred_dfs.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1048de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from individual_rs.individual_rs import IndividualRS\n",
    "from utils.utility_functions import create_per_user_group_choices\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# General pipeline\n",
    "\n",
    "# creating train-test folds\n",
    "# split stratified on the users \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "\n",
    "individual_rs_names = [\n",
    "    \"biasedMf\",\n",
    "    \"implicitMf\"\n",
    "]\n",
    "\n",
    "if group_types == \"SYNTHETIC\":\n",
    "\n",
    "    # load train and test df\n",
    "    print(datetime.now(), \"Load dataset splits\")\n",
    "    train_df = pickle.load(open(preprocessed_dataset_folder+f\"/train_df.pkl\", \"rb\"))\n",
    "    test_df = pickle.load(open(preprocessed_dataset_folder+f\"/test_df.pkl\", \"rb\"))\n",
    "\n",
    "    # getting user-items pairs in the training set\n",
    "    print(datetime.now(), \"Get user-item pairs in the training set\")\n",
    "    train_set_pairs = set(list(zip(train_df['user'].values,train_df['item'].values)))\n",
    "\n",
    "    # create test_complete_df with all the possible user-items pairs in the test_df\n",
    "    print(datetime.now(), \"Create complete_df with all possible pairs\")\n",
    "    user_set = set(test_df['user'].values)\n",
    "    item_set = set(test_df['item'].values)\n",
    "    all_ui_values = list(itertools.product(user_set, item_set))\n",
    "    \n",
    "    test_pred_dfs = {}\n",
    "    for rs_name in individual_rs_names:\n",
    "        individual_rs_path = f\"{preprocessed_dataset_folder}/{rs_name}.pkl\"\n",
    "        test_pred_df = pd.DataFrame(all_ui_values, columns=['user', 'item'])\n",
    "        \n",
    "        # load individual RS\n",
    "        print(datetime.now(), f\"Load individual RS ({rs_name})\")\n",
    "        rs = pickle.load(open(individual_rs_path, \"rb\"))\n",
    "        \n",
    "        # Get predictions\n",
    "        print(datetime.now(), \"Get predictions\")\n",
    "        test_pred_df['predicted_rating'] = rs.predict(test_pred_df)\n",
    "\n",
    "        #correction for train set records (assuming repeated recommendations provide no value, therefore predicted_rating=0)\n",
    "        print(datetime.now(), \"Do the correction\")\n",
    "        train_set_pairs_fixed = list(train_set_pairs.intersection(set(all_ui_values)))\n",
    "        test_pred_df.set_index([\"user\",\"item\"], inplace=True)\n",
    "        test_pred_df.loc[train_set_pairs_fixed,\"predicted_rating\"] = 0.0\n",
    "        test_pred_df.reset_index(inplace=True)\n",
    "        \n",
    "        test_pred_dfs[rs_name] = test_pred_df\n",
    "\n",
    "    path_to_fold = preprocessed_dataset_folder+\"/singlefold\"\n",
    "\n",
    "    if not os.path.exists(path_to_fold):\n",
    "        os.mkdir(path_to_fold)\n",
    "\n",
    "    print(datetime.now(), \"Dump all the results\")\n",
    "    pickle.dump(test_pred_dfs, open(path_to_fold+\"/test_pred_dfs.pkl\", \"wb\"))\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: incorrect config file!\")\n",
    "print(datetime.now(), \"Done!\")\n",
    "\n",
    "test_pred_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b6ca4",
   "metadata": {},
   "source": [
    "# Construct group recommendations\n",
    "Takes a lot of time, only needed if you don't have the group_recommendations files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utility_functions import generate_group_recommendations_forall_groups\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "lst = os.listdir(preprocessed_dataset_folder)\n",
    "\n",
    "for algo_name, tst_pred_df in test_pred_dfs.items():\n",
    "    print(datetime.now(), f\"Processing algorithm {algo_name}\")\n",
    "    path_to_fold = preprocessed_dataset_folder+\"/\"+\"singlefold\"\n",
    "    if group_types == \"SYNTHETIC\":\n",
    "        train_df = pickle.load(open(preprocessed_dataset_folder+\"/train_df.pkl\", \"rb\"))\n",
    "        test_df = pickle.load(open(preprocessed_dataset_folder+\"/test_df.pkl\", \"rb\"))\n",
    "    else:\n",
    "        print(\"ERROR: incorrect config file!\")\n",
    "    \n",
    "    print(datetime.now(), \"Generate GRS for all the aggregation strategies and all the groups\")\n",
    "    \n",
    "    \n",
    "    for sim_name, group_comp in group_compositions.items():\n",
    "        for pred_k in cfg.recommendations_number:\n",
    "            print(f\"Generating for algo_name={algo_name}, sim_name={sim_name}, k={pred_k}\")\n",
    "\n",
    "            group_recommendations = generate_group_recommendations_forall_groups(tst_pred_df, group_comp, pred_k)\n",
    "            print(datetime.now(), f\": Done algo_name={algo_name}, sim_name={sim_name}, k={pred_k}\")\n",
    "\n",
    "            pickle.dump(group_recommendations, open(path_to_fold+\"/\"+f\"predk{pred_k}_group_recommendations_{sim_name}_{algo_name}.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
